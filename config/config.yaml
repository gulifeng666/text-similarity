simillarity:
  edit: Hamming,MLIPNS,
        Levenshtein, DamerauLevenshtein,
        Jaro, JaroWinkler, StrCmp95,
        NeedlemanWunsch, Gotoh, SmithWaterman
  sequence: LCSSeq, LCSStr, RatcliffObershelp
  token: Jaccard, Sorensen, Tversky,
    Overlap, Cosine, Tanimoto, MongeElkan, Bag
  token_vector: WmdDistance
  sentence_vector: VectorCosine,Euclidean,Chebyshev,Minkowski #,Correlation
  compression: ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD
models:
  word2vec_model:
    model0:
       name: Word2Vec
       vectors_path: resource/GoogleNews-vectors-negative300.bin
       support_similarity: token_vector
       return_input: True
    model1:
      name: Word2Vec
      vectors_path: resource/GoogleNews-vectors-negative300.bin
      support_similarity: token_vector
      return_input: False
    model2:
       name: glove
  gensim_model:
    model0:
       name: LdaModel
    model1:
       name: LsiModel
    model2:
       name: TfidfModel
  transformer_model:
    model0:
       name: bert-base-uncased
#pipeline:
#  pipeline2:
#    preprocess: move_stopword
#    model:
#       None:
#    postprocess: None
#    simillarity:
#         compression:
#         edit:
#         sequence:
#         token:
#
#  pipeline3:
#    preprocess: move_stopword
#    model:
#      transformer_model: model0
#    postprocess: MaxPool,SelectOne,Avg
#    simillarity:
#      sentence_vector:
#  pipeline1:
#    preprocess: tokenlize
#    model:
#      word2vec_model: model1
#    postprocess: Avg,MaxPool
#    simillarity:
#      sentence_vector:
#  pipeline0:
#    preprocess: tokenlize
#    model:
#      word2vec_model: model0
#    postprocess: None
#    simillarity:
#      token_vector:
pipeline:
  pipeline3:
    preprocess: None
    model:
      transformer_model: model0
    postprocess: MaxPool,SelectOne,Avg
    simillarity:
      sentence_vector:
  pipeline1:
    preprocess: tokenlize
    model:
      word2vec_model: model1
    postprocess: Avg,MaxPool
    simillarity:
      sentence_vector:
  pipeline0:
    preprocess: tokenlize
    model:
      word2vec_model: model0
    postprocess: None
    simillarity:
      token_vector:
  pipeline2:
    preprocess: None
    model:
       None:
    postprocess: None
    simillarity:
         compression:
         edit:
         sequence:
         token:









