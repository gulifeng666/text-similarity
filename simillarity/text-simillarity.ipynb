{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本相似度初探\n",
    "我们开始讨论测量文本相似度问题，我们的问题是比较一般化的问题:给定训练文本(比如若干句子对），测试文本（比如若干句子对）以及对应的标签（比如句子对的相似程度，数值表示），我们需要利用一些方法在训练数据上进行无监督的训练或不训练\n",
    "，然后在测试文本上进行测试，以确定什么是好的测量文本相似性的方法。\n",
    "给定输入文本，为了输出相似度，我们将文本相似测量方法分为两核心阶段：文本表示阶段和相似性计算阶段，并分别总结一些基础方法，并进行组合，最后利用典型数据:http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark 和\n",
    "https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb 中的SICK和STS数据来进行实验。\n",
    "## 文本表示\n",
    "文本有若干种表示方法，如原始表示，token集合表示，词向量集合表示，句子向量表示等。原始表示就是指文本原始的字符串，token集合是指文本经过分词和其他处理后形成的token集合，词向量集合表示指文本经过词向量模型转后得到的词向量的集合，句子向量表示等指文本经过transformers，主题模型等转换后得到的句子向量表示,我们列出来：\n",
    "\n",
    "|表示方法类别   | 模型       |\n",
    "|---------    | ---       |\n",
    "|原始表示      |           |\n",
    "|token集合表示 |           |\n",
    "|词向量集合表示 |word2vec   |\n",
    "|句子向量表示   |transformer|\n",
    "\n",
    "## 相似度计算\n",
    "我们有许多的相似度计算方法，在https://github.com/life4/textdistance 的划分基础上，我们稍加修改，相似度计算方法可分为基于字符串的，基于句子向量的\n",
    "，基于压缩的，基于编辑的，基于token的和基于词向量的方法。我们不去一一解释这些方法，因为这些基础方法可以很方便的在textdistance包和其他地方搜索到。\n",
    "我们列出来:\n",
    "\n",
    "|相似度计算类别   | 方法列表       |\n",
    "|-----------    |-------       |\n",
    "|基于压缩的      | ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD          |\n",
    "|基于编辑的      |     Hamming,MLIPNS,Levenshtein, DamerauLevenshtein,Jaro, JaroWinkler, StrCmp95,NeedlemanWunsch, Gotoh, SmithWaterman      |\n",
    "|基于词向量的    |WmdDistance   |\n",
    "|基于token的| Jaccard, Sorensen, Tversky,Overlap, Cosine, Tanimoto, MongeElkan, Bag|\n",
    "|基于句子向量的   |Chebyshev, Minkowski, Euclidean|\n",
    "\n",
    "## 预处理和后处理方法\n",
    "除了文本表示和相似度计算方法我们还需要预处理和后处理阶段才能形成完整的相似度计算流程，对于预处理阶段我们仅使用两种：不处理或分词；对于后处理阶段，我们对词向量模型和transformer模型的结果做一些处理以方便能使用不同的相似度计算方法，如：基于词向量的方法或基于句子向量的方法。\n",
    "\n",
    " | 预处理方法       |\n",
    "|-------       |\n",
    "|tokenlize(分词)|\n",
    "|move_stopword|\n",
    "\n",
    "|模型      | 后处理方法       |\n",
    "|-------  |-------       |\n",
    "|word2vec | Avg(平均化所有词向量），Maxpool(对于多个词向量的每个维度取最大的)|\n",
    "|transformer|Avg(平均化所有输出向量），Maxpool(对于多个输出向量的每个维度取最大的)，SelectOne(取第一个输出向量：即[cls]字符代表的向量，其他的不用。)|\n",
    "\n",
    "\n",
    "好了，有了预处理，文本表示，后处理，相似度计算方法，我们就可以依次连接它们，进行不同的组合，形成不同的文本相似度计算的pipeline，即：完整的文本相似度计算流程，随后依此进行试验，来看看不同的pipeline的结果。我们不在此列出具体的pipline了，我们会在后续的实验中展示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=1,\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import gensim.models as gsm\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import textdistance_master.textdistance as td\n",
    "from transformers import AutoModel,AutoConfig,AutoTokenizer,AutoFeatureExtractor,AutoModelForTokenClassification,AutoModelWithLMHead\n",
    "import gensim\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "from random import random, randint\n",
    "from  dataloader.dataloader import DataLoader\n",
    "from models.model import Word2Vec,GensimModel\n",
    "from textdistance_master.textdistance.algorithms.base import Base as _Base, BaseSimilarity as _BaseSimilarity\n",
    "import numpy as np\n",
    "from gensim.corpora import dictionary\n",
    "from functools import reduce\n",
    "from pyemd import emd\n",
    "from gensim import corpora, models, similarities\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#定义Wmd距离\n",
    "class WmdDistance(_BaseSimilarity):\n",
    "    def __call__(self, *sequences):\n",
    "        assert len(sequences)==2\n",
    "        document1,document2 = sequences\n",
    "        docvec1,document1 = document1[:len(document1)//2],document1[len(document1)//2:]\n",
    "        docvec2,document2 = document2[:len(document2)//2],document2[len(document2)//2:]\n",
    "        docdict1 = {word:vec for word,vec  in zip(document1,docvec1 )}\n",
    "        docdict2 = {word:vec for word,vec  in zip(document2,docvec2)}\n",
    "        docdict1.update(docdict2)\n",
    "        docdict = {word:0 for word in document1+document2}\n",
    "        def f(x,y):\n",
    "            x[list(y.keys())[0]] = x.get(list(y.keys())[0],0)+1\n",
    "            return x\n",
    "        d1 =reduce(f,[docdict.copy()]+[{key:1} for key in document1])\n",
    "        d2 =reduce(f,[docdict.copy()]+[{key:1} for key in document2])\n",
    "        # Compute WMD.\n",
    "        distance_matrix = np.array([[np.sqrt(np.sum((docdict1[list(d1.keys())[i]]-docdict1[list(d2.keys())[j]])**2))for i in range(len(d1))] for j in range(len(d2))] ,dtype='float64')\n",
    "        return emd(np.array([value/len(d1) for value in list(d1.values())]), np.array([value/len(d2) for value in list(d2.values())]), distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#封装gensim包方便调用\n",
    "class BaseModel:\n",
    "    def __init__(self,**config):\n",
    "        self.config = config\n",
    "    def load(self):\n",
    "        pass\n",
    "    def train(self):\n",
    "        pass\n",
    "    def __call__(self, sentence_tokens):\n",
    "        pass\n",
    "class Word2Vec(BaseModel):\n",
    "    def __init__(self,**config):\n",
    "        super().__init__(**config)\n",
    "        self.word2vec_path = config['vectors_path']#\"e:/data/word2vec/GoogleNews-vectors-negative300.bin\"\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(self.word2vec_path,binary=True )\n",
    "        self.config = config\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self,sentence_tokens):\n",
    "        veclist = []\n",
    "        wordlist = []\n",
    "        for token in sentence_tokens:\n",
    "          try:\n",
    "            veclist.append(self.word2vec[token.lower()])\n",
    "            wordlist.append(token.lower())\n",
    "\n",
    "          except:\n",
    "            pass\n",
    "        if(self.config['return_input']==True):\n",
    "           veclist=veclist+wordlist\n",
    "\n",
    "\n",
    "        return veclist# for tokens in sentence_tokens]\n",
    "class Glove(Word2Vec):\n",
    "      def __init__(self,**config):\n",
    "          super().__init__(**config)\n",
    "          self.word2vec_path = config['vectors_path']\n",
    "      def __int__(self):\n",
    "          from gensim.test.utils import datapath, get_tmpfile\n",
    "          from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "          glove_file = datapath(self.word2vec_path)\n",
    "          tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "          glove2word2vec(glove_file, tmp_file)\n",
    "          self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)\n",
    "          return self.word2vec\n",
    "class GensimModel(BaseModel):\n",
    "    dic = None\n",
    "    @classmethod\n",
    "    def build_dictionary(cls,sentences_tokens):\n",
    "        cls.dic = corpora.Dictionary(sentences_tokens)\n",
    "        cls.corpus = [cls.dic.doc2bow(tokens) for tokens in sentences_tokens]\n",
    "    def __init__(self,name,**config):\n",
    "        super().__init__(**config)\n",
    "        if hasattr(models,name):\n",
    "           self.model =  getattr(models,name)(self.corpus)\n",
    "        else:\n",
    "           raise NameError\n",
    "    def __call__(self,sentence_tokens):\n",
    "           if(sentence_tokens[0].__class__ != [].__class__):\n",
    "               sentence_tokens = [sentence_tokens]\n",
    "           sentence_bow = [self.dic.doc2bow(tokens) for tokens in sentence_tokens]\n",
    "           res = [self.model[bow] for bow in sentence_bow]\n",
    "           res = [coo_matrix((np.array([item[1] for item in r]), (np.array([0] * len(r)), np.array([item[0] for item in r]))),shape=(1,100)).toarray().T for r in res]\n",
    "           return np.array(res).reshape(len(res),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "class DataLoader:\n",
    "    def load_sts_dataset(self,filename):\n",
    "        # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
    "        # sentences and their human rated similarity score.\n",
    "        sent_pairs = []\n",
    "        with open(filename,encoding='utf-8',mode= \"r\") as f:\n",
    "            for line in f:\n",
    "                ts = line.strip().split(\"\\t\")\n",
    "                sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "        return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "    def load_sts(self):\n",
    "        sts_dev = self.load_sts_dataset(os.path.join(\"data/stsbenchmark\", \"sts-dev.csv\"))\n",
    "        sts_test = self.load_sts_dataset(os.path.join(\"data/stsbenchmark\", \"sts-test.csv\"))\n",
    "        return sts_dev,None,sts_test\n",
    "    def load_sick_datatset(self,file):\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "            lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "            df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "            df['sim'] = pd.to_numeric(df['sim'])\n",
    "            return df\n",
    "    def load_sick(self,):\n",
    "        sick_train =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_train.txt\"))\n",
    "        sick_dev  =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_trial.txt\"))\n",
    "        sick_test  =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_test_annotated.txt\"))\n",
    "        return sick_train,sick_dev,sick_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-acb8640d87fe>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m#定义和加载配置文件\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0myaml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'config/config.yaml'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;31m#config.yaml内容:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m '''\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'config/config.yaml'"
     ]
    }
   ],
   "source": [
    "#定义和加载配置文件\n",
    "config = yaml.load(open('config/config.yaml'))\n",
    "#config.yaml内容:\n",
    "'''\n",
    "simillarity:\n",
    "  edit: Hamming,MLIPNS,\n",
    "        Levenshtein, DamerauLevenshtein,\n",
    "        Jaro, JaroWinkler, StrCmp95,\n",
    "        NeedlemanWunsch, Gotoh, SmithWaterman\n",
    "  sequence: LCSSeq, LCSStr, RatcliffObershelp\n",
    "  token: Jaccard, Sorensen, Tversky,\n",
    "    Overlap, Cosine, Tanimoto, MongeElkan, Bag\n",
    "  token_vector: WmdDistance\n",
    "  sentence_vector: VectorCosine,Euclidean,Chebyshev,Minkowski #,Correlation\n",
    "  compression: ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD\n",
    "models:\n",
    "  word2vec_model:\n",
    "    model0:\n",
    "       name: Word2Vec\n",
    "       vectors_path: resource/GoogleNews-vectors-negative300.bin\n",
    "       support_similarity: token_vector\n",
    "       return_input: True\n",
    "    model1:\n",
    "      name: Word2Vec\n",
    "      vectors_path: resource/GoogleNews-vectors-negative300.bin\n",
    "      support_similarity: token_vector\n",
    "      return_input: False\n",
    "    model2:\n",
    "       name: glove\n",
    "  gensim_model:\n",
    "    model0:\n",
    "       name: LdaModel\n",
    "    model1:\n",
    "       name: LsiModel\n",
    "    model2:\n",
    "       name: TfidfModel\n",
    "  transformer_model:\n",
    "    model0:\n",
    "       name: bert-base-uncased\n",
    "pipeline:\n",
    "\n",
    "  pipeline3:\n",
    "    preprocess: move_stopword\n",
    "    model:\n",
    "      transformer_model: model0\n",
    "    postprocess: MaxPool,SelectOne,Avg\n",
    "    simillarity:\n",
    "      sentence_vector:\n",
    "  pipeline1:\n",
    "    preprocess: tokenlize\n",
    "    model:\n",
    "      word2vec_model: model1\n",
    "    postprocess: Avg,MaxPool\n",
    "    simillarity:\n",
    "      sentence_vector:\n",
    "  pipeline0:\n",
    "    preprocess: tokenlize\n",
    "    model:\n",
    "      word2vec_model: model0\n",
    "    postprocess: None\n",
    "    simillarity:\n",
    "      token_vector:\n",
    "\n",
    "  pipeline2:\n",
    "    preprocess: move_stopword\n",
    "    model:\n",
    "       None:\n",
    "    postprocess: None\n",
    "    simillarity:\n",
    "         edit:\n",
    "         sequence:\n",
    "         token:\n",
    "         compression:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#加载数据和分析数据\n",
    "data_loader = DataLoader()\n",
    "data_train,data_dev,data_test = data_loader.load_sick()\n",
    "data_train\n",
    "#print('平均长度为%s'%str(sum([len(x.split() )for x in data_test['sent_2']])/len(data_test)))\n",
    "#plt.hist([len(x.split() )for x in data_train['sent_2']])\n",
    "#data_test['sim'].hist()\n",
    "#plt.scatter(list(range(len(data_test))),sorted(data_test['sim'].tolist()))\n",
    "#data_test['len'] = data_test.apply(lambda x:(len(x[1].split())+len(x[2].split()))/2,1)\n",
    "#data_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train,data_dev,data_test = data_loader.load_sick()\n",
    "print('平均长度为%s'%str(sum([len(x.split() )for x in data_test['sent_2']])/len(data_test)))\n",
    "data_train\n",
    "#plt.hist([len(x.split() )for x in data_train['sent_2']])\n",
    "data_test['sim'].hist()\n",
    "#plt.scatter(list(range(len(data_test))),sorted(data_test['sim'].tolist()))\n",
    "#data_test.describe()\n",
    "#data_test['len'] = data_test.apply(lambda x:(len(x[0].split())+len(x[1].split()))/2,1)\n",
    "#data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#组合不同的文本相似度计算pipeline，计算得到结果，pipeline 名称为多个阶段具体方法名称的连接：pipeline_key+preprocess_name+model_name+str(model_number)+postprocess_name+sim_method_class+sim_method_name\n",
    "GensimModel.build_dictionary([nltk.word_tokenize(sentence) for sentence in data_train['sent_1'].tolist()+data_train['sent_2'].tolist()+data_dev['sent_1'].tolist()+data_dev['sent_2'].tolist()])\n",
    "dataframe = pd.DataFrame(index = ['pearson','spearmanr'])\n",
    "totalnum=0\n",
    "track = True\n",
    "for pipeline_key,pipeline_value in config['pipeline'].items():\n",
    "    for preprocess_name in map(lambda x:x.strip(),pipeline_value['preprocess'].split(',')):\n",
    "        if(preprocess_name=='None'):\n",
    "            preprocess = lambda x:x\n",
    "        elif(preprocess_name == 'move_stopword'):\n",
    "            preprocess = lambda x:x\n",
    "        elif(preprocess_name == 'tokenlize'):\n",
    "            preprocess = lambda x:nltk.word_tokenize(x)\n",
    "        else:\n",
    "            raise NameError\n",
    "        from scipy.sparse import coo_matrix\n",
    "        for model_name in map(lambda x:x.strip(),pipeline_value['model'].keys()):\n",
    "            model_numbers = map(lambda x: x.strip(), pipeline_value['model'][model_name].split(',')) if model_name != 'None' else [None]\n",
    "            for model_number in model_numbers:\n",
    "                if(model_name=='None'):\n",
    "                    model = lambda x :x\n",
    "                elif(model_name=='gensim_model'):\n",
    "                    model = GensimModel(config['models'][model_name][model_number]['name'])\n",
    "                elif(model_name=='transformer_model'):\n",
    "                    transformer_model = AutoModel.from_pretrained(config['models'][model_name][model_number]['name'])#bert-base-uncased')\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(config['models'][model_name][model_number]['name'])\n",
    "                    #model(**tokenizer('i want', return_tensors='pt'))\n",
    "                    model = lambda x:transformer_model(**tokenizer(x,return_tensors='pt'))[0]\n",
    "                elif(model_name == 'word2vec_model'):\n",
    "                    model = Word2Vec(**config['models'][model_name][model_number])\n",
    "                else:\n",
    "                    raise NameError\n",
    "                for postprocess_name in map(lambda x:x.strip(),pipeline_value['postprocess'].split(',')) :\n",
    "                    if(postprocess_name == 'None'):\n",
    "                        postprocess = lambda x:x\n",
    "                    elif(postprocess_name=='Avg'):\n",
    "                        postprocess = lambda x:np.mean(x.detach().numpy() if type(x)==torch.Tensor else x,-2)\n",
    "                    elif(postprocess_name=='MaxPool'):\n",
    "                        postprocess = lambda x:np.max(x.detach().numpy() if type(x)==torch.Tensor else x,-2)\n",
    "                    elif(postprocess_name=='SelectOne'):\n",
    "                        postprocess = lambda x:x.detach().numpy()[:,0]\n",
    "                    else:\n",
    "                      raise NameError\n",
    "                    for sim_method_class in tqdm(map(lambda x: x.strip(), pipeline_value['simillarity'].keys())):\n",
    "                        for sim_method_name in  tqdm(map(lambda x: x.strip(),config['simillarity'][sim_method_class].split(','))):\n",
    "                           sim = getattr(td, sim_method_name)() if 'WMD' not in sim_method_name else getattr(td, sim_method_name)(model)\n",
    "                           sim = sim.similarity if 'vector' not in sim_method_class else sim\n",
    "                           res = [sim(postprocess(model(preprocess(sentencepair[0]))),postprocess(model(preprocess(sentencepair[1])))) for sentencepair in zip(data_test[\"sent_1\"].tolist(),data_test[\"sent_2\"].tolist())]\n",
    "                           res = list(map(lambda x:-x,res)) if 'vector'  in sim_method_class else res\n",
    "                           pearson_correlation = scipy.stats.pearsonr(res,data_test['sim'].tolist())[0]\n",
    "                           spearmanr_correlation = scipy.stats.spearmanr(res,data_test['sim'].tolist())[0]\n",
    "                           if(track):\n",
    "                               with mlflow.start_run():\n",
    "                                   log_param('dataset','sick')\n",
    "                                   log_param('pipelinekey',pipeline_key)\n",
    "                                   log_param('preprocess',preprocess_name)\n",
    "                                   log_param('model_key',model_name)\n",
    "                                   log_param('modelnumber',str(model_number))\n",
    "                                   log_param('postprocess',postprocess_name)\n",
    "                                   log_param('sim_method',sim_method_class)\n",
    "                                   log_param('sim_method_name',sim_method_name)\n",
    "                                   log_metric('pearson',pearson_correlation)\n",
    "                                   log_metric('spearmanr',spearmanr_correlation)\n",
    "                               dataframe [str(totalnum)+'_'+pipeline_key+preprocess_name+model_name+str(model_number)+postprocess_name+sim_method_class+sim_method_name] = [pearson_correlation,spearmanr_correlation]\n",
    "                               totalnum+=1\n",
    "plt.rcParams['font.size'] = 7\n",
    "fig = dataframe.T.plot.bar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们来看看实验结果，实验结果并不非常一致，我们发现在sick数据集上transformer的效果很好，结合各种后处理方法的效果变化不大比较稳定,word2vec的不同组合之间的差异较大,在所有变化中word2vec结合wmd距离的效果最好，\n",
    "非学习方法中，基于压缩的BWT,ZLI方法表现突出。在sts数据集中,实验结果大不相同，word2vec的各种变化效果最好，特别是结合cosine，结合wmd距离的效果并不好;transformer的不同组合之间变化很大，在所有变化中transformer结合maxpool,cosine的效果最好;非学习方法中，ZLI,LZM的效果最好。\n",
    "在两个数据上，对于transformer来说先用maxpool再结合相似度计算方法的效果要好于平均化和取cls字符向量;对于word2vec来说并没有在两个数据集上一直表现最好的组合方法。\n",
    "\n",
    "tansformer和word2vec均需要结合其他方法进行实验，不同的结合方法效果差异很大，同类模型的变体之间的效果差异很大，没有哪一类模型效果绝对的好，各自有各自的长处。\n",
    "\n",
    "非学习方法不许要调参，只是大部分非学习方法效果一般，基于压缩的[ZLI方法](https://en.wikipedia.org/wiki/Zlib)在两个数据集上都取得了很好的效果，而且不需要调参，它应当是一个值得考虑的baseline。\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    src=\"output/stsplot.png\" width = \"65%\" alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">\n",
    "      sts数据集\n",
    "  \t</div>\n",
    "</center>\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    src=\"output/sick.png\" width = \"65%\" alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">\n",
    "      sick数据集\n",
    "  \t</div>\n",
    "</center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}