{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本相似度初探\n",
    "我们开始讨论测量文本相似度问题，我们的问题是比较一般化的问题:给定训练文本(比如若干句子对），测试文本（比如若干句子对）以及对应的标签（比如句子对的相似程度，数值表示），我们需要利用一些方法在训练数据上进行无监督的训练或不训练\n",
    "，然后在测试文本上进行测试，以确定什么是好的测量文本相似性的方法。\n",
    "给定输入文本，为了输出相似度，我们将文本相似测量方法分为两核心阶段：文本表示阶段和相似性计算阶段，并分别总结一些基础方法，并进行组合，最后利用典型数据:http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark 和\n",
    "https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb 中的SICK和STS数据来进行实验。\n",
    "## 文本表示\n",
    "文本有若干种表示方法，如原始表示，token集合表示，词向量集合表示，句子向量表示等。原始表示就是指文本原始的字符串，token集合是指文本经过分词和其他处理后形成的token集合，词向量集合表示指文本经过词向量模型转后得到的词向量的集合，句子向量表示等指文本经过transformers，主题模型等转换后得到的句子向量表示,我们列出来：\n",
    "\n",
    "|表示方法类别   | 模型       |\n",
    "|---------    | ---       |\n",
    "|原始表示      |           |\n",
    "|token集合表示 |           |\n",
    "|词向量集合表示 |word2vec   |\n",
    "|句子向量表示   |transformer|\n",
    "\n",
    "## 相似度计算\n",
    "我们有许多的相似度计算方法，在https://github.com/life4/textdistance 的划分基础上，我们稍加修改，相似度计算方法可分为基于字符串的，基于句子向量的\n",
    "，基于压缩的，基于编辑的，基于token的和基于词向量的方法。我们不去一一解释这些方法，因为这些基础方法可以很方便的在textdistance包和其他地方搜索到。\n",
    "我们列出来:\n",
    "\n",
    "|相似度计算类别   | 方法列表       |\n",
    "|-----------    |-------       |\n",
    "|基于压缩的      | ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD          |\n",
    "|基于编辑的      |     Hamming,MLIPNS,Levenshtein, DamerauLevenshtein,Jaro, JaroWinkler, StrCmp95,NeedlemanWunsch, Gotoh, SmithWaterman      |\n",
    "|基于词向量的    |WmdDistance   |\n",
    "|基于token的| Jaccard, Sorensen, Tversky,Overlap, Cosine, Tanimoto, MongeElkan, Bag|\n",
    "|基于句子向量的   |Chebyshev, Minkowski, Euclidean|\n",
    "\n",
    "## 预处理和后处理方法\n",
    "除了文本表示和相似度计算方法我们还需要预处理和后处理阶段才能形成完整的相似度计算流程，对于预处理阶段我们仅使用两种：不处理或分词；对于后处理阶段，我们对词向量模型和transformer模型的结果做一些处理以方便能使用不同的相似度计算方法，如：基于词向量的方法或基于句子向量的方法。\n",
    "\n",
    " | 预处理方法       |\n",
    "|-------       |\n",
    "|tokenlize(分词)|\n",
    "|move_stopword|\n",
    "\n",
    "|模型      | 后处理方法       |\n",
    "|-------  |-------       |\n",
    "|word2vec | Avg(平均化所有词向量），Maxpool(对于多个词向量的每个维度取最大的)|\n",
    "|transformer|Avg(平均化所有输出向量），Maxpool(对于多个输出向量的每个维度取最大的)，SelectOne(取第一个输出向量：即[cls]字符代表的向量，其他的不用。)|\n",
    "\n",
    "\n",
    "好了，有了预处理，文本表示，后处理，相似度计算方法，我们就可以依次连接它们，进行不同的组合，形成不同的文本相似度计算的pipeline，即：完整的文本相似度计算流程，随后依此进行试验，来看看不同的pipeline的结果。我们不在此列出具体的pipline了，我们会在后续的实验中展示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=1,\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import gensim.models as gsm\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import textdistance_master.textdistance as td\n",
    "from transformers import AutoModel,AutoConfig,AutoTokenizer,AutoFeatureExtractor,AutoModelForTokenClassification,AutoModelWithLMHead\n",
    "import gensim\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "from random import random, randint\n",
    "from  dataloader.dataloader import DataLoader\n",
    "from models.model import Word2Vec,GensimModel\n",
    "from textdistance_master.textdistance.algorithms.base import Base as _Base, BaseSimilarity as _BaseSimilarity\n",
    "import numpy as np\n",
    "from gensim.corpora import dictionary\n",
    "from functools import reduce\n",
    "from pyemd import emd\n",
    "from gensim import corpora, models, similarities\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#定义Wmd距离\n",
    "class WmdDistance(_BaseSimilarity):\n",
    "    def __call__(self, *sequences):\n",
    "        assert len(sequences)==2\n",
    "        document1,document2 = sequences\n",
    "        docvec1,document1 = document1[:len(document1)//2],document1[len(document1)//2:]\n",
    "        docvec2,document2 = document2[:len(document2)//2],document2[len(document2)//2:]\n",
    "        docdict1 = {word:vec for word,vec  in zip(document1,docvec1 )}\n",
    "        docdict2 = {word:vec for word,vec  in zip(document2,docvec2)}\n",
    "        docdict1.update(docdict2)\n",
    "        docdict = {word:0 for word in document1+document2}\n",
    "        def f(x,y):\n",
    "            x[list(y.keys())[0]] = x.get(list(y.keys())[0],0)+1\n",
    "            return x\n",
    "        d1 =reduce(f,[docdict.copy()]+[{key:1} for key in document1])\n",
    "        d2 =reduce(f,[docdict.copy()]+[{key:1} for key in document2])\n",
    "        # Compute WMD.\n",
    "        distance_matrix = np.array([[np.sqrt(np.sum((docdict1[list(d1.keys())[i]]-docdict1[list(d2.keys())[j]])**2))for i in range(len(d1))] for j in range(len(d2))] ,dtype='float64')\n",
    "        return emd(np.array([value/len(d1) for value in list(d1.values())]), np.array([value/len(d2) for value in list(d2.values())]), distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#封装gensim包方便调用\n",
    "class BaseModel:\n",
    "    def __init__(self,**config):\n",
    "        self.config = config\n",
    "    def load(self):\n",
    "        pass\n",
    "    def train(self):\n",
    "        pass\n",
    "    def __call__(self, sentence_tokens):\n",
    "        pass\n",
    "class Word2Vec(BaseModel):\n",
    "    def __init__(self,**config):\n",
    "        super().__init__(**config)\n",
    "        self.word2vec_path = config['vectors_path']#\"e:/data/word2vec/GoogleNews-vectors-negative300.bin\"\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(self.word2vec_path,binary=True )\n",
    "        self.config = config\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self,sentence_tokens):\n",
    "        veclist = []\n",
    "        wordlist = []\n",
    "        for token in sentence_tokens:\n",
    "          try:\n",
    "            veclist.append(self.word2vec[token.lower()])\n",
    "            wordlist.append(token.lower())\n",
    "\n",
    "          except:\n",
    "            pass\n",
    "        if(self.config['return_input']==True):\n",
    "           veclist=veclist+wordlist\n",
    "\n",
    "\n",
    "        return veclist# for tokens in sentence_tokens]\n",
    "class Glove(Word2Vec):\n",
    "      def __init__(self,**config):\n",
    "          super().__init__(**config)\n",
    "          self.word2vec_path = config['vectors_path']\n",
    "      def __int__(self):\n",
    "          from gensim.test.utils import datapath, get_tmpfile\n",
    "          from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "          glove_file = datapath(self.word2vec_path)\n",
    "          tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "          glove2word2vec(glove_file, tmp_file)\n",
    "          self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)\n",
    "          return self.word2vec\n",
    "class GensimModel(BaseModel):\n",
    "    dic = None\n",
    "    @classmethod\n",
    "    def build_dictionary(cls,sentences_tokens):\n",
    "        cls.dic = corpora.Dictionary(sentences_tokens)\n",
    "        cls.corpus = [cls.dic.doc2bow(tokens) for tokens in sentences_tokens]\n",
    "    def __init__(self,name,**config):\n",
    "        super().__init__(**config)\n",
    "        if hasattr(models,name):\n",
    "           self.model =  getattr(models,name)(self.corpus)\n",
    "        else:\n",
    "           raise NameError\n",
    "    def __call__(self,sentence_tokens):\n",
    "           if(sentence_tokens[0].__class__ != [].__class__):\n",
    "               sentence_tokens = [sentence_tokens]\n",
    "           sentence_bow = [self.dic.doc2bow(tokens) for tokens in sentence_tokens]\n",
    "           res = [self.model[bow] for bow in sentence_bow]\n",
    "           res = [coo_matrix((np.array([item[1] for item in r]), (np.array([0] * len(r)), np.array([item[0] for item in r]))),shape=(1,100)).toarray().T for r in res]\n",
    "           return np.array(res).reshape(len(res),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "class DataLoader:\n",
    "    def load_sts_dataset(self,filename):\n",
    "        # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
    "        # sentences and their human rated similarity score.\n",
    "        sent_pairs = []\n",
    "        with open(filename,encoding='utf-8',mode= \"r\") as f:\n",
    "            for line in f:\n",
    "                ts = line.strip().split(\"\\t\")\n",
    "                sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "        return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "    def load_sts(self):\n",
    "        sts_dev = self.load_sts_dataset(os.path.join(\"data/stsbenchmark\", \"sts-dev.csv\"))\n",
    "        sts_test = self.load_sts_dataset(os.path.join(\"data/stsbenchmark\", \"sts-test.csv\"))\n",
    "        return sts_dev,None,sts_test\n",
    "    def load_sick_datatset(self,file):\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()[1:]\n",
    "            lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "            lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "            df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "            df['sim'] = pd.to_numeric(df['sim'])\n",
    "            return df\n",
    "    def load_sick(self,):\n",
    "        sick_train =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_train.txt\"))\n",
    "        sick_dev  =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_trial.txt\"))\n",
    "        sick_test  =  self.load_sick_datatset(os.path.join(\"e:data\", \"SICK_test_annotated.txt\"))\n",
    "        return sick_train,sick_dev,sick_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nsimillarity:\\n  edit: Hamming,MLIPNS,\\n        Levenshtein, DamerauLevenshtein,\\n        Jaro, JaroWinkler, StrCmp95,\\n        NeedlemanWunsch, Gotoh, SmithWaterman\\n  sequence: LCSSeq, LCSStr, RatcliffObershelp\\n  token: Jaccard, Sorensen, Tversky,\\n    Overlap, Cosine, Tanimoto, MongeElkan, Bag\\n  token_vector: WmdDistance\\n  sentence_vector: VectorCosine,Euclidean,Chebyshev,Minkowski #,Correlation\\n  compression: ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD\\nmodels:\\n  word2vec_model:\\n    model0:\\n       name: Word2Vec\\n       vectors_path: resource/GoogleNews-vectors-negative300.bin\\n       support_similarity: token_vector\\n       return_input: True\\n    model1:\\n      name: Word2Vec\\n      vectors_path: resource/GoogleNews-vectors-negative300.bin\\n      support_similarity: token_vector\\n      return_input: False\\n    model2:\\n       name: glove\\n  gensim_model:\\n    model0:\\n       name: LdaModel\\n    model1:\\n       name: LsiModel\\n    model2:\\n       name: TfidfModel\\n  transformer_model:\\n    model0:\\n       name: bert-base-uncased\\npipeline:\\n\\n  pipeline3:\\n    preprocess: move_stopword\\n    model:\\n      transformer_model: model0\\n    postprocess: MaxPool,SelectOne,Avg\\n    simillarity:\\n      sentence_vector:\\n  pipeline1:\\n    preprocess: tokenlize\\n    model:\\n      word2vec_model: model1\\n    postprocess: Avg,MaxPool\\n    simillarity:\\n      sentence_vector:\\n  pipeline0:\\n    preprocess: tokenlize\\n    model:\\n      word2vec_model: model0\\n    postprocess: None\\n    simillarity:\\n      token_vector:\\n\\n  pipeline2:\\n    preprocess: move_stopword\\n    model:\\n       None:\\n    postprocess: None\\n    simillarity:\\n         edit:\\n         sequence:\\n         token:\\n         compression:\\n\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义和加载配置文件\n",
    "config = yaml.load(open('config/config.yaml'))\n",
    "#config.yaml内容:\n",
    "'''\n",
    "simillarity:\n",
    "  edit: Hamming,MLIPNS,\n",
    "        Levenshtein, DamerauLevenshtein,\n",
    "        Jaro, JaroWinkler, StrCmp95,\n",
    "        NeedlemanWunsch, Gotoh, SmithWaterman\n",
    "  sequence: LCSSeq, LCSStr, RatcliffObershelp\n",
    "  token: Jaccard, Sorensen, Tversky,\n",
    "    Overlap, Cosine, Tanimoto, MongeElkan, Bag\n",
    "  token_vector: WmdDistance\n",
    "  sentence_vector: VectorCosine,Euclidean,Chebyshev,Minkowski #,Correlation\n",
    "  compression: ArithNCD,LZMANCD,BZ2NCD,RLENCD,BWTRLENCD,ZLIBNCD,SqrtNCD,EntropyNCD\n",
    "models:\n",
    "  word2vec_model:\n",
    "    model0:\n",
    "       name: Word2Vec\n",
    "       vectors_path: resource/GoogleNews-vectors-negative300.bin\n",
    "       support_similarity: token_vector\n",
    "       return_input: True\n",
    "    model1:\n",
    "      name: Word2Vec\n",
    "      vectors_path: resource/GoogleNews-vectors-negative300.bin\n",
    "      support_similarity: token_vector\n",
    "      return_input: False\n",
    "    model2:\n",
    "       name: glove\n",
    "  gensim_model:\n",
    "    model0:\n",
    "       name: LdaModel\n",
    "    model1:\n",
    "       name: LsiModel\n",
    "    model2:\n",
    "       name: TfidfModel\n",
    "  transformer_model:\n",
    "    model0:\n",
    "       name: bert-base-uncased\n",
    "pipeline:\n",
    "\n",
    "  pipeline3:\n",
    "    preprocess: move_stopword\n",
    "    model:\n",
    "      transformer_model: model0\n",
    "    postprocess: MaxPool,SelectOne,Avg\n",
    "    simillarity:\n",
    "      sentence_vector:\n",
    "  pipeline1:\n",
    "    preprocess: tokenlize\n",
    "    model:\n",
    "      word2vec_model: model1\n",
    "    postprocess: Avg,MaxPool\n",
    "    simillarity:\n",
    "      sentence_vector:\n",
    "  pipeline0:\n",
    "    preprocess: tokenlize\n",
    "    model:\n",
    "      word2vec_model: model0\n",
    "    postprocess: None\n",
    "    simillarity:\n",
    "      token_vector:\n",
    "\n",
    "  pipeline2:\n",
    "    preprocess: move_stopword\n",
    "    model:\n",
    "       None:\n",
    "    postprocess: None\n",
    "    simillarity:\n",
    "         edit:\n",
    "         sequence:\n",
    "         token:\n",
    "         compression:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        idx                                             sent_1  \\\n0         1  A group of kids is playing in a yard and an ol...   \n1         2  A group of children is playing in the house an...   \n2         3  The young boys are playing outdoors and the ma...   \n3         5  The kids are playing outdoors near a man with ...   \n4         9  The young boys are playing outdoors and the ma...   \n...     ...                                                ...   \n4495   9993                    A door is being opened by a man   \n4496   9997                   Someone is boiling okra in a pot   \n4497   9998  The man is singing heartily and playing the gu...   \n4498   9999        A man in blue has a yellow ball in the mitt   \n4499  10000               Three dogs are resting on a sidewalk   \n\n                                                 sent_2  sim         label  \n0     A group of boys in a yard is playing and a man...  4.5     NEUTRAL\\n  \n1     A group of kids is playing in a yard and an ol...  3.2     NEUTRAL\\n  \n2     The kids are playing outdoors near a man with ...  4.7  ENTAILMENT\\n  \n3     A group of kids is playing in a yard and an ol...  3.4     NEUTRAL\\n  \n4     A group of kids is playing in a yard and an ol...  3.7     NEUTRAL\\n  \n...                                                 ...  ...           ...  \n4495  A bald man in a band is playing guitar in the ...  1.1     NEUTRAL\\n  \n4496                   The man is not playing the drums  1.0     NEUTRAL\\n  \n4497  A bicyclist is holding a bike over his head in...  1.0     NEUTRAL\\n  \n4498                      A man is jumping rope outside  1.2     NEUTRAL\\n  \n4499         The woman with a knife is slicing a pepper  1.0     NEUTRAL\\n  \n\n[4500 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>sent_1</th>\n      <th>sent_2</th>\n      <th>sim</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A group of kids is playing in a yard and an ol...</td>\n      <td>A group of boys in a yard is playing and a man...</td>\n      <td>4.5</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>A group of children is playing in the house an...</td>\n      <td>A group of kids is playing in a yard and an ol...</td>\n      <td>3.2</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>The young boys are playing outdoors and the ma...</td>\n      <td>The kids are playing outdoors near a man with ...</td>\n      <td>4.7</td>\n      <td>ENTAILMENT\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>The kids are playing outdoors near a man with ...</td>\n      <td>A group of kids is playing in a yard and an ol...</td>\n      <td>3.4</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>The young boys are playing outdoors and the ma...</td>\n      <td>A group of kids is playing in a yard and an ol...</td>\n      <td>3.7</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4495</th>\n      <td>9993</td>\n      <td>A door is being opened by a man</td>\n      <td>A bald man in a band is playing guitar in the ...</td>\n      <td>1.1</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>4496</th>\n      <td>9997</td>\n      <td>Someone is boiling okra in a pot</td>\n      <td>The man is not playing the drums</td>\n      <td>1.0</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>4497</th>\n      <td>9998</td>\n      <td>The man is singing heartily and playing the gu...</td>\n      <td>A bicyclist is holding a bike over his head in...</td>\n      <td>1.0</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>4498</th>\n      <td>9999</td>\n      <td>A man in blue has a yellow ball in the mitt</td>\n      <td>A man is jumping rope outside</td>\n      <td>1.2</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n    <tr>\n      <th>4499</th>\n      <td>10000</td>\n      <td>Three dogs are resting on a sidewalk</td>\n      <td>The woman with a knife is slicing a pepper</td>\n      <td>1.0</td>\n      <td>NEUTRAL\\n</td>\n    </tr>\n  </tbody>\n</table>\n<p>4500 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载数据和分析数据\n",
    "data_loader = DataLoader()\n",
    "data_train,data_dev,data_test = data_loader.load_sick()\n",
    "data_train\n",
    "#print('平均长度为%s'%str(sum([len(x.split() )for x in data_test['sent_2']])/len(data_test)))\n",
    "#plt.hist([len(x.split() )for x in data_train['sent_2']])\n",
    "#data_test['sim'].hist()\n",
    "#plt.scatter(list(range(len(data_test))),sorted(data_test['sim'].tolist()))\n",
    "#data_test['len'] = data_test.apply(lambda x:(len(x[1].split())+len(x[2].split()))/2,1)\n",
    "#data_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均长度为9.524660036533387\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1d9eac61630>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train,data_dev,data_test = data_loader.load_sick()\n",
    "print('平均长度为%s'%str(sum([len(x.split() )for x in data_test['sent_2']])/len(data_test)))\n",
    "data_train\n",
    "#plt.hist([len(x.split() )for x in data_train['sent_2']])\n",
    "data_test['sim'].hist()\n",
    "#plt.scatter(list(range(len(data_test))),sorted(data_test['sim'].tolist()))\n",
    "#data_test.describe()\n",
    "#data_test['len'] = data_test.apply(lambda x:(len(x[0].split())+len(x[1].split()))/2,1)\n",
    "#data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [02:00, 120.71s/it]\u001B[A\n",
      "2it [04:57, 153.72s/it]\u001B[A\n",
      "3it [04:59, 84.28s/it] \u001B[Ad:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:3010: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "\n",
      "4it [05:00, 51.49s/it]\u001B[A\n",
      "5it [05:02, 33.73s/it]\u001B[A\n",
      "6it [05:05, 23.22s/it]\u001B[A\n",
      "7it [05:06, 16.03s/it]\u001B[A\n",
      "8it [05:08, 38.52s/it]\u001B[A\n",
      "1it [05:08, 308.12s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  1.54it/s]\u001B[A\n",
      "2it [00:01,  1.45it/s]\u001B[A\n",
      "3it [01:25, 39.01s/it]\u001B[A\n",
      "4it [02:24, 46.56s/it]\u001B[A\n",
      "5it [02:25, 30.21s/it]\u001B[A\n",
      "6it [02:26, 20.38s/it]\u001B[A\n",
      "7it [02:28, 14.48s/it]\u001B[A\n",
      "8it [04:00, 39.16s/it]\u001B[A\n",
      "9it [10:33, 149.58s/it]\u001B[A\n",
      "10it [13:17, 79.74s/it] \u001B[A\n",
      "2it [18:25, 595.96s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:40, 40.41s/it]\u001B[A\n",
      "2it [00:41, 17.47s/it]\u001B[A\n",
      "3it [00:44, 14.83s/it]\u001B[A\n",
      "3it [19:10, 344.15s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  1.24it/s]\u001B[A\n",
      "2it [00:01,  1.40it/s]\u001B[A\n",
      "3it [00:02,  1.33it/s]\u001B[A\n",
      "4it [00:03,  1.30it/s]\u001B[A\n",
      "5it [00:03,  1.25it/s]\u001B[A\n",
      "6it [00:04,  1.16it/s]\u001B[A\n",
      "7it [05:43, 111.36s/it]\u001B[A\n",
      "8it [05:44, 43.08s/it] \u001B[A\n",
      "4it [24:54, 373.68s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001D9EAE871D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mgaierror\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    170\u001B[0m             conn = connection.create_connection(\n\u001B[1;32m--> 171\u001B[1;33m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001B[0m\u001B[0;32m    172\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mport\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSOCK_STREAM\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m         \u001B[0maf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\socket.py\u001B[0m in \u001B[0;36mgetaddrinfo\u001B[1;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[0;32m    747\u001B[0m     \u001B[0maddrlist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 748\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_socket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mport\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    749\u001B[0m         \u001B[0maf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mgaierror\u001B[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    599\u001B[0m                                                   \u001B[0mbody\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 600\u001B[1;33m                                                   chunked=chunked)\n\u001B[0m\u001B[0;32m    601\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 343\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_conn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    344\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_validate_conn\u001B[1;34m(self, conn)\u001B[0m\n\u001B[0;32m    848\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'sock'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# AppEngine might not have  `.sock`\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 849\u001B[1;33m             \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    850\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;31m# Add certificate verification\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 314\u001B[1;33m         \u001B[0mconn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_new_conn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    315\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    179\u001B[0m             raise NewConnectionError(\n\u001B[1;32m--> 180\u001B[1;33m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001B[0m\u001B[0;32m    181\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNewConnectionError\u001B[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x000001D9EAE871D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    444\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 445\u001B[1;33m                     \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    446\u001B[0m                 )\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    637\u001B[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001B[1;32m--> 638\u001B[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001B[0m\u001B[0;32m    639\u001B[0m             \u001B[0mretries\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001B[0m in \u001B[0;36mincrement\u001B[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[0;32m    397\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mnew_retry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_exhausted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 398\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mMaxRetryError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_pool\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merror\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mResponseError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcause\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    399\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001D9EAE871D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-826c8b1dd76f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     25\u001B[0m                 \u001B[1;32melif\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[1;33m==\u001B[0m\u001B[1;34m'transformer_model'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m                     \u001B[0mtransformer_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'models'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodel_number\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;31m#bert-base-uncased')\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'models'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmodel_number\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m                     \u001B[1;31m#model(**tokenizer('i want', return_tensors='pt'))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m                     \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mtransformer_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mreturn_tensors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'pt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\code\\python\\text-simillarity\\transformers\\models\\auto\\tokenization_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    569\u001B[0m             \u001B[0mtokenizer_class_py\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer_class_fast\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTOKENIZER_MAPPING\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    570\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtokenizer_class_fast\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0muse_fast\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mtokenizer_class_py\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 571\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mtokenizer_class_fast\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    572\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    573\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mtokenizer_class_py\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\code\\python\\text-simillarity\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1646\u001B[0m             \u001B[1;31m# At this point pretrained_model_name_or_path is either a directory or a model identifier name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1647\u001B[0m             fast_tokenizer_file = get_fast_tokenizer_file(\n\u001B[1;32m-> 1648\u001B[1;33m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrevision\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrevision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1649\u001B[0m             )\n\u001B[0;32m   1650\u001B[0m             additional_files_names = {\n",
      "\u001B[1;32mE:\\code\\python\\text-simillarity\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mget_fast_tokenizer_file\u001B[1;34m(path_or_repo, revision, use_auth_token)\u001B[0m\n\u001B[0;32m   3406\u001B[0m     \"\"\"\n\u001B[0;32m   3407\u001B[0m     \u001B[1;31m# Inspect all files from the repo/folder.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3408\u001B[1;33m     \u001B[0mall_files\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_list_of_files\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_or_repo\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrevision\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrevision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muse_auth_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_auth_token\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3409\u001B[0m     \u001B[0mtokenizer_files_map\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3410\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mfile_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_files\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\code\\python\\text-simillarity\\transformers\\file_utils.py\u001B[0m in \u001B[0;36mget_list_of_files\u001B[1;34m(path_or_repo, revision, use_auth_token)\u001B[0m\n\u001B[0;32m   1692\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1693\u001B[0m     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\n\u001B[1;32m-> 1694\u001B[1;33m         \u001B[0mpath_or_repo\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrevision\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrevision\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1695\u001B[0m     )\n\u001B[0;32m   1696\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrfilename\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmodel_info\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msiblings\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\hf_api.py\u001B[0m in \u001B[0;36mmodel_info\u001B[1;34m(self, repo_id, revision, token)\u001B[0m\n\u001B[0;32m    245\u001B[0m             \u001B[1;33m{\u001B[0m\u001B[1;34m\"authorization\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"Bearer {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    246\u001B[0m         )\n\u001B[1;32m--> 247\u001B[1;33m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    248\u001B[0m         \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_for_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m         \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m     \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msetdefault\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'allow_redirects'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'get'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[1;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    510\u001B[0m         }\n\u001B[0;32m    511\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 512\u001B[1;33m         \u001B[0mresp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    513\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    514\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    620\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    621\u001B[0m         \u001B[1;31m# Send the request\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 622\u001B[1;33m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    623\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    624\u001B[0m         \u001B[1;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    511\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mSSLError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 513\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    514\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    515\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mClosedPoolError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mConnectionError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001D9EAE871D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "#组合不同的文本相似度计算pipeline，计算得到结果，pipeline 名称为多个阶段具体方法名称的连接：pipeline_key+preprocess_name+model_name+str(model_number)+postprocess_name+sim_method_class+sim_method_name\n",
    "GensimModel.build_dictionary([nltk.word_tokenize(sentence) for sentence in data_train['sent_1'].tolist()+data_train['sent_2'].tolist()+data_dev['sent_1'].tolist()+data_dev['sent_2'].tolist()])\n",
    "dataframe = pd.DataFrame(index = ['pearson','spearmanr'])\n",
    "totalnum=0\n",
    "track = True\n",
    "for pipeline_key,pipeline_value in config['pipeline'].items():\n",
    "    for preprocess_name in map(lambda x:x.strip(),pipeline_value['preprocess'].split(',')):\n",
    "        if(preprocess_name=='None'):\n",
    "            preprocess = lambda x:x\n",
    "        elif(preprocess_name == 'move_stopword'):\n",
    "            preprocess = lambda x:x\n",
    "        elif(preprocess_name == 'tokenlize'):\n",
    "            preprocess = lambda x:nltk.word_tokenize(x)\n",
    "        else:\n",
    "            raise NameError\n",
    "        from scipy.sparse import coo_matrix\n",
    "        for model_name in map(lambda x:x.strip(),pipeline_value['model'].keys()):\n",
    "            model_numbers = map(lambda x: x.strip(), pipeline_value['model'][model_name].split(',')) if model_name != 'None' else [None]\n",
    "            for model_number in model_numbers:\n",
    "                if(model_name=='None'):\n",
    "                    model = lambda x :x\n",
    "                elif(model_name=='gensim_model'):\n",
    "                    model = GensimModel(config['models'][model_name][model_number]['name'])\n",
    "                elif(model_name=='transformer_model'):\n",
    "                    transformer_model = AutoModel.from_pretrained(config['models'][model_name][model_number]['name'])#bert-base-uncased')\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(config['models'][model_name][model_number]['name'])\n",
    "                    #model(**tokenizer('i want', return_tensors='pt'))\n",
    "                    model = lambda x:transformer_model(**tokenizer(x,return_tensors='pt'))[0]\n",
    "                elif(model_name == 'word2vec_model'):\n",
    "                    model = Word2Vec(**config['models'][model_name][model_number])\n",
    "                else:\n",
    "                    raise NameError\n",
    "                for postprocess_name in map(lambda x:x.strip(),pipeline_value['postprocess'].split(',')) :\n",
    "                    if(postprocess_name == 'None'):\n",
    "                        postprocess = lambda x:x\n",
    "                    elif(postprocess_name=='Avg'):\n",
    "                        postprocess = lambda x:np.mean(x.detach().numpy() if type(x)==torch.Tensor else x,-2)\n",
    "                    elif(postprocess_name=='MaxPool'):\n",
    "                        postprocess = lambda x:np.max(x.detach().numpy() if type(x)==torch.Tensor else x,-2)\n",
    "                    elif(postprocess_name=='SelectOne'):\n",
    "                        postprocess = lambda x:x.detach().numpy()[:,0]\n",
    "                    else:\n",
    "                      raise NameError\n",
    "                    for sim_method_class in tqdm(map(lambda x: x.strip(), pipeline_value['simillarity'].keys())):\n",
    "                        for sim_method_name in  tqdm(map(lambda x: x.strip(),config['simillarity'][sim_method_class].split(','))):\n",
    "                           sim = getattr(td, sim_method_name)() if 'WMD' not in sim_method_name else getattr(td, sim_method_name)(model)\n",
    "                           sim = sim.similarity if 'vector' not in sim_method_class else sim\n",
    "                           res = [sim(postprocess(model(preprocess(sentencepair[0]))),postprocess(model(preprocess(sentencepair[1])))) for sentencepair in zip(data_test[\"sent_1\"].tolist(),data_test[\"sent_2\"].tolist())]\n",
    "                           res = list(map(lambda x:-x,res)) if 'vector'  in sim_method_class else res\n",
    "                           pearson_correlation = scipy.stats.pearsonr(res,data_test['sim'].tolist())[0]\n",
    "                           spearmanr_correlation = scipy.stats.spearmanr(res,data_test['sim'].tolist())[0]\n",
    "                           if(track):\n",
    "                               with mlflow.start_run():\n",
    "                                   log_param('dataset','sick')\n",
    "                                   log_param('pipelinekey',pipeline_key)\n",
    "                                   log_param('preprocess',preprocess_name)\n",
    "                                   log_param('model_key',model_name)\n",
    "                                   log_param('modelnumber',str(model_number))\n",
    "                                   log_param('postprocess',postprocess_name)\n",
    "                                   log_param('sim_method',sim_method_class)\n",
    "                                   log_param('sim_method_name',sim_method_name)\n",
    "                                   log_metric('pearson',pearson_correlation)\n",
    "                                   log_metric('spearmanr',spearmanr_correlation)\n",
    "                               dataframe [str(totalnum)+'_'+pipeline_key+preprocess_name+model_name+str(model_number)+postprocess_name+sim_method_class+sim_method_name] = [pearson_correlation,spearmanr_correlation]\n",
    "                               totalnum+=1\n",
    "plt.rcParams['font.size'] = 7\n",
    "fig = dataframe.T.plot.bar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们来看看实验结果，实验结果并不非常一致，我们发现在sick数据集上transformer的效果很好，结合各种后处理方法的效果变化不大比较稳定,word2vec的不同组合之间的差异较大,在所有变化中word2vec结合wmd距离的效果最好，\n",
    "非学习方法中，基于压缩的BWT,ZLI方法表现突出。在sts数据集中,实验结果大不相同，word2vec的各种变化效果最好，特别是结合cosine，结合wmd距离的效果并不好;transformer的不同组合之间变化很大，在所有变化中transformer结合maxpool,cosine的效果最好;非学习方法中，ZLI,LZM的效果最好。\n",
    "在两个数据上，对于transformer来说先用maxpool再结合相似度计算方法的效果要好于平均化和取cls字符向量;对于word2vec来说并没有在两个数据集上一直表现最好的组合方法。\n",
    "\n",
    "tansformer和word2vec均需要结合其他方法进行实验，不同的结合方法效果差异很大，同类模型的变体之间的效果差异很大，没有哪一类模型效果绝对的好，各自有各自的长处。\n",
    "\n",
    "非学习方法不许要调参，只是大部分非学习方法效果一般，基于压缩的[ZLI方法](https://en.wikipedia.org/wiki/Zlib)在两个数据集上都取得了很好的效果，而且不需要调参，它应当是一个值得考虑的baseline。\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    src=\"output/stsplot.png\" width = \"65%\" alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">\n",
    "      sts数据集\n",
    "  \t</div>\n",
    "</center>\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    src=\"output/sick.png\" width = \"65%\" alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">\n",
    "      sick数据集\n",
    "  \t</div>\n",
    "</center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}